# -*- coding: utf-8 -*-
"""fork-of-rl-proj-inv-pendulum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mkvxOcOo_4nuaggi3lu85hZhuYjfXhOm
"""

import numpy as np
import matplotlib.pyplot as plt
from queue import PriorityQueue

G = 10
L = 1
M = 1
MAX_SPEED = 8
MAX_TORQUE = 2
DT = 0.05

def clip(x, min_val, max_val):
      """
      function specified in problem description to clip values within range
      """
      if (x < min_val):
        return min_val
      elif (x > max_val):
        return max_val
      return x

class InvPendulum():
  def __init__(self, w_n_bins=55, w_dot_n_bins=55, action_n_bins=10):
    self.end = False
    self.count = 0
    self.w_n_bins = w_n_bins
    self.w_dot_n_bins = w_dot_n_bins
    self.action_n_bins = action_n_bins
    self.w = None
    self.w_dot = None
    self.state = None

  def init_state(self):
    """
    initializing state using given condiions
    """
    self.w = np.random.uniform(((-5/6)*np.pi), ((5/6)*np.pi))
    self.w_dot = np.random.uniform(-1, 1)
    return np.array([self.w, self.w_dot])

  def reset(self):
    self.end = False
    self.count = 0

  def discretize_state(self, state):
    w, w_dot = state
    w_norm = (w + np.pi) % (2*np.pi) - np.pi
    w_bins = np.linspace(-np.pi, np.pi, self.w_n_bins)
    w_dot_bins = np.linspace(-MAX_SPEED, MAX_SPEED, self.w_dot_n_bins)
    w_disc = np.digitize(w_norm, w_bins) - 1
    w_dot_clipped = clip(w_dot, -MAX_SPEED, MAX_SPEED)
    w_dot_disc = np.digitize(w_dot_clipped, w_dot_bins) - 1

    return ((w_disc, w_dot_disc))

  def action_bin(self, a_bin):
    action_bins = np.linspace(-MAX_TORQUE, MAX_TORQUE, self.action_n_bins)
    l = action_bins[a_bin]
    if a_bin + 1 == self.action_n_bins:
      u = action_bins[a_bin]
    else:
      u = action_bins[a_bin+1]
    return (l+u)/2

  def get_step(self, action):
    a = clip(action, -MAX_TORQUE, MAX_TORQUE)
    w2 = ((3*G*np.sin(self.w))/(2*L)) + ((3*a)/(M*(L**2)))
    w1 = clip(self.w + w2*DT, -MAX_SPEED, MAX_SPEED)
    w = self.w + w1*DT
    w_norm = ((self.w+np.pi)%(2*np.pi)) - np.pi
    w_norm_new = ((w+np.pi)%(2*np.pi)) - np.pi

    r = -(w_norm**2 + 0.1*self.w_dot**2 + 0.001*a**2)

    self.w = w_norm_new
    self.w_dot = w1
    self.count += 1

    if self.count >= 200:
      self.end = True

    return np.array([self.w, self.w_dot]), r

class PrioritizedSweeping():
    def __init__(self, env=InvPendulum(), alpha=0.2, epsilon=0, n=5, theta=0, episodes=1500, gamma=1):
        self.env = env
        self.model = {}
        self.predecessors = {}
        self.q = np.zeros((self.env.w_n_bins, self.env.w_dot_n_bins, self.env.action_n_bins))
        self.gamma = gamma
        self.PQueue = PriorityQueue()
        self.theta = theta
        self.alpha = alpha
        self.epsilon = epsilon
        self.num_episodes = episodes
        self.n = n
        self.returns_per_episode = []

    def best_action(self, state):
        best_a = []
        best_q = np.max(self.q[state])
        for action in range(self.env.action_n_bins):
            if self.q[state][action] == best_q:
                best_a.append(action)
        return best_a

    def epsilon_greedy_policy(self, state):
        best_a = self.best_action(state)
        action_prob = {}
        for action in range(self.env.action_n_bins):
            if action in best_a:
                action_prob[action] = (1 - self.epsilon) / len(best_a) + (self.epsilon / self.env.action_n_bins)
            else:
                action_prob[action] = (self.epsilon / self.env.action_n_bins)
        action = np.random.choice(list(action_prob.keys()), p=list(action_prob.values()))
        return action

    def algo(self):
        for episode in range(self.num_episodes):
            state = self.env.init_state()
            state_disc = self.env.discretize_state(state)
            state_disc_tuple = tuple(state_disc)
            self.env.reset()
            print(f"Episode {episode + 1}")

            total_return = 0
            while not self.env.end:

                action = self.epsilon_greedy_policy(state_disc_tuple)
                action_cont = self.env.action_bin(action)

                next_state, reward = self.env.get_step(action_cont)
                next_state_disc = self.env.discretize_state(next_state)
                next_state_disc_tuple = tuple(next_state_disc)

                if state_disc_tuple not in self.model:
                    self.model[state_disc_tuple] = {}
                self.model[state_disc_tuple][action] = (reward, next_state_disc_tuple)

                if next_state_disc_tuple not in self.predecessors:
                    self.predecessors[next_state_disc_tuple] = [(state_disc_tuple, action)]
                else:
                    self.predecessors[next_state_disc_tuple].append((state_disc_tuple, action))

                max_a = self.best_action(next_state_disc_tuple)[0]
                p = abs(reward + self.gamma * self.q[next_state_disc_tuple][max_a] - self.q[state_disc_tuple][action])
                if p > self.theta:
                    self.PQueue.put((-p, (state_disc_tuple, action)))

                total_return += reward

                state = next_state
                state_disc_tuple = next_state_disc_tuple

            for _ in range(self.n):
                if self.PQueue.empty():
                    break

                (s, a) = self.PQueue.get()[1]

                if s not in self.model:
                    continue
                if a not in self.model[s]:
                    continue

                reward, s_next = self.model[s][a]
                max_a = self.best_action(s_next)[0]

                self.q[s][a] += self.alpha * (reward + self.gamma * self.q[s_next][max_a] - self.q[s][a])

                if s not in self.predecessors:
                    continue
                for (pred_s, pred_a) in self.predecessors[s]:
                    if pred_s not in self.model:
                        continue
                    if pred_a not in self.model[pred_s]:
                        continue

                    pred_r, _ = self.model[pred_s][pred_a]
                    max_a = self.best_action(s)[0]
                    p = abs(pred_r + self.gamma * self.q[s][max_a] - self.q[pred_s][pred_a])
                    if p > self.theta:
                        self.PQueue.put((-p, (pred_s, pred_a)))
            self.returns_per_episode.append(total_return)

ps = PrioritizedSweeping(episodes=1000)
ps.algo()

import matplotlib.pyplot as plt
returns = ps.returns_per_episode
plt.plot(range(len(returns)), returns)
plt.xlabel("Episode")
plt.ylabel("Total Return")
plt.title("Learning Curve")
plt.show()

