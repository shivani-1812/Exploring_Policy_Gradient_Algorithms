# -*- coding: utf-8 -*-
"""rl-proj-500-epsilon-0-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vS9s5NCb7VMfWkzztnaILXwMTAw_xiTz
"""

import numpy as np
import os
from queue import PriorityQueue
import matplotlib.pyplot as plt

optimal_policy = [['AR', 'AD', 'AL', 'AD', 'AD'],
                  ['AR', 'AR', 'AR', 'AR', 'AD'],
                  ['AU', None, None, None, 'AD'],
                  ['AU', 'AL', None, 'AR', 'AD'],
                  ['AU', 'AR', 'AR', 'AR', 'G']]

optimal_v = np.array([[2.6638, 2.9969, 2.8117, 3.6671, 4.8497],
                      [2.9713, 3.5101, 4.0819, 4.8497, 7.1648],
                      [2.5936, 0.000, 0.000, 0.000, 8.4687],
                      [2.0992, 1.0849, 0.000, 8.6097, 9.5269],
                      [1.0849, 4.9465, 8.4687, 9.5269, 0.0000]])

def calc_mse(values, optimal_v=optimal_v):
  #(1/S)*(\sum_s((v1(s)-v2(s))^2))
  #v1- optimal_v, v2- calc_v
  return np.mean((optimal_v - values) ** 2)

class Grid():
  def __init__(self, gamma=0.925):
    self.gamma = gamma
    self.rows = 5
    self.cols = 5
    self.forbidden_furniture = [(2,1), (2,2), (2,3), (3,2)]
    self.monsters = [(0,3), (4,1)]
    self.food = (4,4)
    self.actions = {'AU': (-1, 0), 'AD': (1, 0), 'AL': (0, -1), 'AR': (0, 1)}
    self.left_of_action = {'AU': (0, -1), 'AD': (0, 1), 'AL': (1, 0), 'AR': (-1, 0)}
    self.right_of_action = {'AU': (0, 1), 'AD': (0, -1), 'AL': (-1, 0), 'AR': (1, 0)}
    self.terminal_states = [(4,4)]
    states = [(i, j) for i in range(5) for j in range(5)]
    self.states = [state for state in states if state not in self.forbidden_furniture]

  def init_state(self):
    indices = np.arange(len(self.states))
    ind = np.random.choice(indices)
    init_s = self.states[ind]
    return init_s


  def is_valid_state(self, state):
    return 0 <= state[0] < self.rows and 0 <= state[1] < self.cols and state not in self.forbidden_furniture

  def get_next_state(self, state, action):
    next_states = []
    probs = []
    intended_state = (state[0] + self.actions[action][0], state[1] + self.actions[action][1])
    if self.is_valid_state(intended_state):
      next_states.append(intended_state)
      probs.append(0.7)
    else:
      next_states.append(state)
      probs.append(0.7)

    confused_left = (state[0] + self.left_of_action[action][0], state[1] + self.left_of_action[action][1])

    if self.is_valid_state(confused_left):
      next_states.append(confused_left)
      probs.append(0.12)

    else:
      next_states.append(state)
      probs.append(0.12)

    confused_right = (state[0] + self.right_of_action[action][0], state[1] + self.right_of_action[action][1])

    if self.is_valid_state(confused_right):
      next_states.append(confused_right)
      probs.append(0.12)

    else:
      next_states.append(state)
      probs.append(0.12)

    next_states.append(state)
    probs.append(0.06)
    ind = np.random.choice(np.arange(len(next_states)), p=probs)

    return next_states[ind]

  def get_reward(self, next_state):
    if next_state == self.food:
      return 10
    elif next_state in self.monsters:
      return -8
    else:
      return -0.05

  def is_terminal(self, state):
    return state == self.food

class PrioritizedSweeping():
  def __init__(self, env=Grid(), alpha=0.05, epsilon=0.1, n=5, theta=0, episodes=500, gamma=0.925):
    self.env = env
    self.state = np.empty(2)
    self.action = None
    self.model = {}
    self.predecessors = {}
    self.q = {(r, c): {action: 0.0 for action in list(self.env.actions.keys())}
                        for r in range(self.env.rows) for c in range(self.env.cols)}
    self.values = np.zeros((env.rows, env.cols))
    self.gamma = gamma
    self.PQueue = PriorityQueue()
    self.theta = theta
    self.alpha = alpha
    self.epsilon = epsilon
    self.num_episodes = episodes
    self.n = n
    self.num_actions_per_episode = []
    self.num_actions = 0
    self.mse_vals = []


  def best_action(self, state):
    best_a = None
    best_q = float('-inf')
    for action in list(self.env.actions.keys()):
      q_val = self.q[state][action]
      if q_val > best_q:
        best_q = q_val
        best_a = action
    return best_a

  def epsilon_greedy_policy(self):
    action_prob = {}
    best_a = self.best_action(self.state)
    for action in list(self.env.actions.keys()):
      if action in best_a:
        action_prob[action] = 1 - self.epsilon + (self.epsilon / len(list(self.env.actions.keys())))
      else:
        action_prob[action] = (self.epsilon / len(list(self.env.actions.keys())))
    action = np.random.choice(list(action_prob.keys()), p=list(action_prob.values()))
    return action

  def find_vals(self):
    #values is max a (q)
    for state in list(self.q.keys()):
      a = self.best_action(state)
      self.values[state[0]][state[1]] = self.q[state][a]

  def algo(self):
    for i in range(self.num_episodes):
      self.state = self.env.init_state()
      while not self.env.is_terminal(self.state):
        self.action = self.epsilon_greedy_policy()
        self.num_actions += 1
        next_state = self.env.get_next_state(self.state, self.action)
        reward = self.env.get_reward(next_state)
        max_a = self.best_action(state=next_state)
        if self.state not in self.model.keys():
          self.model[self.state] = {}
        self.model[self.state][self.action] = (reward, next_state)
        if next_state not in self.predecessors.keys():
          self.predecessors[next_state] = [(self.state, self.action)]
        else:
          self.predecessors[next_state].append((self.state, self.action))
        p = abs(reward + (self.gamma*self.q[next_state][max_a]) - self.q[self.state][self.action])
        if p >self.theta:
          self.PQueue.put((-p, (self.state, self.action)))
        self.state = next_state

        for j in range(self.n):
          if self.PQueue.empty():
            break
          (s, a) = self.PQueue.get()[1]
          if s not in list(self.model.keys()):
            continue
          if a not in list(self.model[s].keys()):
            continue
          r, s_next = self.model[s][a]
          max_a = self.best_action(s_next)
          self.q[s][a] += self.alpha*(r + (self.gamma*self.q[s_next][max_a]) - self.q[s][a])

          if s not in list(self.predecessors.keys()):
            continue

          else:
            for (_s, _a) in self.predecessors[s]:
              if _s not in list(self.model.keys()):
                continue
              if _a not in list(self.model[_s].keys()):
                continue
              _r, _ = self.model[_s][_a]
              max_a = self.best_action(s)
              p = abs(_r + (self.gamma*self.q[s][max_a]) - self.q[_s][_a])
              if p > self.theta:
                self.PQueue.put((-p, (self.state, self.action)))
      self.num_actions_per_episode.append(self.num_actions)
      self.find_vals()
      self.mse_vals.append(calc_mse(self.values))

import matplotlib.pyplot as plt

env = Grid()
num_actions = []
mse_vals = []
for i in range(20):
    print("Run:",(i+1))
    ps = PrioritizedSweeping(episodes=500, epsilon=0.2)
    ps.algo()
    num_actions.append(ps.num_actions_per_episode)
    mse_vals.append(ps.mse_vals)

avg_total_actions = np.mean(num_actions, axis=0)
y = range(1, len(avg_total_actions) + 1)

plt.figure(figsize=(10, 6))
plt.plot(avg_total_actions, y, label='Learning Curve', color='blue')
plt.xlabel('Total Actions Taken')
plt.ylabel('Episodes Completed')
plt.title('Learning Curve')
plt.grid(True)
plt.legend()
plt.show()

avg_mse = np.mean(mse_vals, axis=0)

x = range(1, len(avg_mse) + 1)
plt.figure(figsize=(10, 6))
plt.plot(x, avg_mse, label='Learning Curve', color='blue')
plt.xlabel('Number of Episodes')
plt.ylabel('MSE')
plt.title('Learning Curve')
plt.grid(True)
plt.legend()
plt.show()