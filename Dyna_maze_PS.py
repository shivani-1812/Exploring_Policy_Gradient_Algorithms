# -*- coding: utf-8 -*-
"""rl-proj-dyna-maze.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18DeAtWcFV9rS40JowUC8RsDH5U4iXtq6
"""

import numpy as np
import os
from queue import PriorityQueue
import matplotlib.pyplot as plt

class Maze():
    def __init__(self):
        self.rows = 6
        self.cols = 9
        self.goal_state = (0,8)
        self.block_states = [(1, 2), (2, 2), (3, 2), (0, 7), (1, 7), (2, 7), (4, 5)]
        self.actions = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}
        self.state = (0,0)
        self.states = np.zeros((self.rows, self.cols))
        self.end = False

    def init_state(self):
        self.end = False
        self.state = (0,2)

    def is_valid_state(self, state):
        return 0 <= state[0] < self.rows and 0 <= state[1] < self.cols and state not in self.block_states

    def get_next_state(self, action):
        # state = self.state
        next_state = (self.state[0] + self.actions[action][0], self.state[1] + self.actions[action][1])
        if self.is_valid_state(next_state):
            self.state = next_state
        return self.state

    def get_reward(self):
        if self.state == self.goal_state:
            self.end = True
            return 1
        else:
            return 0

class PrioritizedSweeping():
  def __init__(self, env=Maze(), alpha=0.05, epsilon=0.1, n=5, theta=0, episodes=50, gamma=0.925):
    self.env = env
    self.state = (0,0)
    self.action = None
    self.model = {}
    self.predecessors = {}
    self.q = {(r, c): {action: 0.0 for action in list(self.env.actions.keys())}
                        for r in range(self.env.rows) for c in range(self.env.cols)}
    self.values = np.zeros((env.rows, env.cols))
    self.gamma = gamma
    self.PQueue = PriorityQueue()
    self.theta = theta
    self.alpha = alpha
    self.epsilon = epsilon
    self.num_episodes = episodes
    self.n = n
    self.num_actions_per_episode = []
    self.num_actions = 0


  def best_action(self, state):
    best_a = None
    best_q = float('-inf')
    for action in list(self.env.actions.keys()):
      q_val = self.q[state][action]
      if q_val > best_q:
        best_q = q_val
        best_a = [action]
      if q_val == best_q:
        best_a.append(action)
    return best_a

  def epsilon_greedy_policy(self):
    action_prob = {}
    best_a = self.best_action(self.state)
    for action in list(self.env.actions.keys()):
      if action in best_a:
        action_prob[action] = ((1 - self.epsilon)/(len(best_a)-1)) + (self.epsilon / len(list(self.env.actions.keys())))
      else:
        action_prob[action] = (self.epsilon / len(list(self.env.actions.keys())))

    action = np.random.choice(list(action_prob.keys()), p=list(action_prob.values()))
    return action

  def algo(self):
    for i in range(self.num_episodes):
      # print("Episode:",i+1)
      self.env.init_state()
      self.state = self.env.state
      while not self.env.end:
        self.action = self.epsilon_greedy_policy()
        # print("s,a:", self.state, self.action)
        self.num_actions += 1
        next_state = self.env.get_next_state(self.action)
        reward = self.env.get_reward()
        max_a = self.best_action(state=next_state)[0]
        if self.state not in self.model.keys():
          self.model[self.state] = {}
        self.model[self.state][self.action] = (reward, next_state)
        if next_state not in self.predecessors.keys():
          self.predecessors[next_state] = [(self.state, self.action)]
        else:
          self.predecessors[next_state].append((self.state, self.action))
        p = abs(reward + (self.gamma*self.q[next_state][max_a]) - self.q[self.state][self.action])
        if p >self.theta:
          self.PQueue.put((-p, (self.state, self.action)))
        self.state = next_state

        for j in range(self.n):
          if self.PQueue.empty():
            break
          (s, a) = self.PQueue.get()[1]
          if s not in list(self.model.keys()):
            continue
          if a not in list(self.model[s].keys()):
            continue
          r, s_next = self.model[s][a]
          max_a = self.best_action(s_next)[0]
          self.q[s][a] += self.alpha*(r + (self.gamma*self.q[s_next][max_a]) - self.q[s][a])

          if s not in list(self.predecessors.keys()):
            continue

          else:
            for (_s, _a) in self.predecessors[s]:
              if _s not in list(self.model.keys()):
                continue
              if _a not in list(self.model[_s].keys()):
                continue
              _r, _ = self.model[_s][_a]
              max_a = self.best_action(s)[0]
              p = abs(_r + (self.gamma*self.q[s][max_a]) - self.q[_s][_a])
              if p > self.theta:
                self.PQueue.put((-p, (self.state, self.action)))
      self.num_actions_per_episode.append(self.num_actions)

# ps = PrioritizedSweeping(episodes=50)
# ps.algo()

num_actions = []
for i in range(20):
  print("Run:",i+1)
  ps = PrioritizedSweeping()
  ps.algo()
  num_actions.append(ps.num_actions_per_episode)

# num_actions = ps.num_actions_per_episode
avg_num_actions = np.mean(num_actions, axis=0)
y = range(1, len(avg_num_actions) + 1)

plt.figure(figsize=(10, 6))
plt.plot(avg_num_actions, y, label='Learning Curve', color='blue')
plt.xlabel('Total Actions Taken')
plt.ylabel('Episodes Completed')
plt.title('Learning Curve')
plt.grid(True)
plt.legend()
plt.show()

env = Maze()
policy = np.empty((env.rows, env.cols), dtype=object)
# policy[4][4] = 'G'
for state in list(ps.q.keys()):
  if state == env.goal_state:
    policy[state] = 'G'
    continue
  elif state in env.block_states:
    continue
  policy[state] = ps.best_action(state)

arrow_mapping = {
    'U': '↑',
    'D': '↓',
    'L': '←',
    'R': '→',
    'G': 'G',
    None: 'X'
}

def print_policy(policy):
  print("\nThe greedy policy is:")
  for row in policy:
      for action in row[0]:
          arrow = arrow_mapping[action]
          print(f"{arrow}  ", end="")
      print()

print_policy(policy)

